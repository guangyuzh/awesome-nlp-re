reading wordembedding
reading training data
2017-12-09T19:39:18.793647: step 200, softmax_loss 101.806, acc 0.36
2017-12-09T19:39:43.723280: step 400, softmax_loss 68.5508, acc 0.5
2017-12-09T19:40:07.979418: step 600, softmax_loss 55.8337, acc 0.64
2017-12-09T19:40:32.657035: step 800, softmax_loss 50.8549, acc 0.68
2017-12-09T19:40:57.286733: step 1000, softmax_loss 56.3387, acc 0.6
2017-12-09T19:41:21.485616: step 1200, softmax_loss 49.6894, acc 0.6
2017-12-09T19:41:46.061059: step 1400, softmax_loss 42.6942, acc 0.76
2017-12-09T19:42:10.552797: step 1600, softmax_loss 51.8455, acc 0.64
2017-12-09T19:42:34.641383: step 1800, softmax_loss 43.8872, acc 0.74
2017-12-09T19:42:59.394552: step 2000, softmax_loss 42.3792, acc 0.74
2017-12-09T19:43:23.788568: step 2200, softmax_loss 49.4858, acc 0.6
2017-12-09T19:43:47.947732: step 2400, softmax_loss 42.9017, acc 0.74
2017-12-09T19:44:12.720664: step 2600, softmax_loss 54.9759, acc 0.64
2017-12-09T19:44:37.446260: step 2800, softmax_loss 47.2878, acc 0.56
2017-12-09T19:45:01.508936: step 3000, softmax_loss 50.0669, acc 0.66
2017-12-09T19:45:25.796272: step 3200, softmax_loss 51.5894, acc 0.6
2017-12-09T19:45:50.600012: step 3400, softmax_loss 48.0455, acc 0.72
2017-12-09T19:46:14.679357: step 3600, softmax_loss 31.9131, acc 0.78
2017-12-09T19:46:38.963438: step 3800, softmax_loss 28.5755, acc 0.76
2017-12-09T19:47:03.960977: step 4000, softmax_loss 56.6205, acc 0.64
2017-12-09T19:47:28.011188: step 4200, softmax_loss 41.9455, acc 0.7
2017-12-09T19:47:52.428053: step 4400, softmax_loss 41.968, acc 0.68
2017-12-09T19:48:17.231424: step 4600, softmax_loss 39.5793, acc 0.7
2017-12-09T19:48:41.307140: step 4800, softmax_loss 39.5366, acc 0.72
2017-12-09T19:49:05.357893: step 5000, softmax_loss 49.2805, acc 0.64
2017-12-09T19:49:30.278239: step 5200, softmax_loss 41.2158, acc 0.7
2017-12-09T19:49:54.453634: step 5400, softmax_loss 33.2191, acc 0.78
2017-12-09T19:50:18.941895: step 5600, softmax_loss 33.9111, acc 0.72
2017-12-09T19:50:43.688298: step 5800, softmax_loss 42.8819, acc 0.64
2017-12-09T19:51:07.786574: step 6000, softmax_loss 41.2619, acc 0.62
2017-12-09T19:51:32.207642: step 6200, softmax_loss 31.9004, acc 0.74
2017-12-09T19:51:57.056603: step 6400, softmax_loss 28.0057, acc 0.74
2017-12-09T19:52:21.310895: step 6600, softmax_loss 27.9566, acc 0.82
2017-12-09T19:52:45.407426: step 6800, softmax_loss 40.6345, acc 0.66
2017-12-09T19:53:09.954227: step 7000, softmax_loss 33.7963, acc 0.7
2017-12-09T19:53:34.649485: step 7200, softmax_loss 36.1508, acc 0.76
2017-12-09T19:53:58.756793: step 7400, softmax_loss 43.5316, acc 0.66
2017-12-09T19:54:23.100251: step 7600, softmax_loss 36.9074, acc 0.72
2017-12-09T19:54:47.915985: step 7800, softmax_loss 47.8769, acc 0.68
2017-12-09T19:55:12.121430: step 8000, softmax_loss 38.5708, acc 0.78
2017-12-09T19:55:36.673460: step 8200, softmax_loss 47.794, acc 0.7
2017-12-09T19:56:01.968671: step 8400, softmax_loss 30.7973, acc 0.8
2017-12-09T19:56:26.098181: step 8600, softmax_loss 36.0215, acc 0.6
2017-12-09T19:56:50.510135: step 8800, softmax_loss 27.3945, acc 0.76
2017-12-09T19:57:15.464764: step 9000, softmax_loss 40.4259, acc 0.76
2017-12-09T19:57:39.560108: step 9200, softmax_loss 38.1537, acc 0.62
2017-12-09T19:58:03.889140: step 9400, softmax_loss 39.3969, acc 0.8
2017-12-09T19:58:28.653239: step 9600, softmax_loss 38.0607, acc 0.64
2017-12-09T19:58:52.693872: step 9800, softmax_loss 35.7373, acc 0.72
2017-12-09T19:59:17.135457: step 10000, softmax_loss 45.7745, acc 0.62
2017-12-09T19:59:42.035272: step 10200, softmax_loss 36.5098, acc 0.72
2017-12-09T20:00:06.177916: step 10400, softmax_loss 36.8042, acc 0.8
2017-12-09T20:00:30.290222: step 10600, softmax_loss 30.0942, acc 0.72
2017-12-09T20:00:54.825008: step 10800, softmax_loss 35.6412, acc 0.72
2017-12-09T20:01:19.341017: step 11000, softmax_loss 39.4544, acc 0.72
2017-12-09T20:01:43.428795: step 11200, softmax_loss 28.5898, acc 0.8
2017-12-09T20:02:08.264734: step 11400, softmax_loss 33.2055, acc 0.74
2017-12-09T20:02:32.807264: step 11600, softmax_loss 38.4205, acc 0.58
2017-12-09T20:02:56.837444: step 11800, softmax_loss 31.1799, acc 0.8
2017-12-09T20:03:21.103495: step 12000, softmax_loss 34.5661, acc 0.78
2017-12-09T20:03:45.987799: step 12200, softmax_loss 39.762, acc 0.74
2017-12-09T20:04:10.092484: step 12400, softmax_loss 45.1956, acc 0.64
2017-12-09T20:04:34.242192: step 12600, softmax_loss 26.7739, acc 0.82
2017-12-09T20:04:59.258221: step 12800, softmax_loss 45.257, acc 0.58
2017-12-09T20:05:23.413428: step 13000, softmax_loss 35.1758, acc 0.66
2017-12-09T20:05:47.465263: step 13200, softmax_loss 23.8106, acc 0.78
2017-12-09T20:06:12.346246: step 13400, softmax_loss 25.9718, acc 0.82
2017-12-09T20:06:36.735431: step 13600, softmax_loss 32.4392, acc 0.8
2017-12-09T20:07:00.950682: step 13800, softmax_loss 31.1881, acc 0.76
2017-12-09T20:07:25.404192: step 14000, softmax_loss 33.9818, acc 0.66
2017-12-09T20:07:50.115313: step 14200, softmax_loss 39.9768, acc 0.64
2017-12-09T20:08:14.192049: step 14400, softmax_loss 25.7282, acc 0.76
2017-12-09T20:08:38.689272: step 14600, softmax_loss 34.1146, acc 0.72
2017-12-09T20:09:03.254453: step 14800, softmax_loss 22.3323, acc 0.8
2017-12-09T20:09:27.329684: step 15000, softmax_loss 28.2881, acc 0.76
2017-12-09T20:09:51.872013: step 15200, softmax_loss 16.6979, acc 0.9
2017-12-09T20:10:16.645475: step 15400, softmax_loss 31.7901, acc 0.7
2017-12-09T20:10:40.775582: step 15600, softmax_loss 25.1234, acc 0.82
2017-12-09T20:11:05.025276: step 15800, softmax_loss 28.4063, acc 0.72
2017-12-09T20:11:29.860182: step 16000, softmax_loss 26.3248, acc 0.82
saving model
have saved model to ./model/ATT_GRU_model-16000
2017-12-09T20:11:56.566902: step 16200, softmax_loss 32.5651, acc 0.74
2017-12-09T20:12:20.984317: step 16400, softmax_loss 30.7012, acc 0.74
2017-12-09T20:12:45.683071: step 16600, softmax_loss 30.2507, acc 0.74
2017-12-09T20:13:09.818190: step 16800, softmax_loss 23.8029, acc 0.8
2017-12-09T20:13:34.555810: step 17000, softmax_loss 23.6184, acc 0.76
2017-12-09T20:13:59.232578: step 17200, softmax_loss 14.5207, acc 0.9
2017-12-09T20:14:23.601725: step 17400, softmax_loss 23.0096, acc 0.8
2017-12-09T20:14:48.297263: step 17600, softmax_loss 32.1718, acc 0.8
2017-12-09T20:15:12.750682: step 17800, softmax_loss 28.2365, acc 0.74
2017-12-09T20:15:36.917564: step 18000, softmax_loss 37.1214, acc 0.62
saving model
have saved model to ./model/ATT_GRU_model-18000
2017-12-09T20:16:03.905171: step 18200, softmax_loss 41.2545, acc 0.68
2017-12-09T20:16:28.038359: step 18400, softmax_loss 18.0987, acc 0.82
2017-12-09T20:16:52.563980: step 18600, softmax_loss 29.4056, acc 0.76
2017-12-09T20:17:17.272453: step 18800, softmax_loss 29.4091, acc 0.76
2017-12-09T20:17:41.359738: step 19000, softmax_loss 34.1464, acc 0.78
2017-12-09T20:18:05.403197: step 19200, softmax_loss 23.1575, acc 0.86
2017-12-09T20:18:30.493702: step 19400, softmax_loss 28.575, acc 0.76
2017-12-09T20:18:54.917280: step 19600, softmax_loss 24.4797, acc 0.8
2017-12-09T20:19:19.154206: step 19800, softmax_loss 25.3497, acc 0.82
2017-12-09T20:19:43.661041: step 20000, softmax_loss 20.9258, acc 0.86
saving model
have saved model to ./model/ATT_GRU_model-20000
2017-12-09T20:20:10.440616: step 20200, softmax_loss 18.45, acc 0.86
