reading wordembedding
reading training data
2017-12-15T20:29:16.679431: step 200, softmax_loss 74.3791, acc 0.58
2017-12-15T20:29:52.614874: step 400, softmax_loss 46.8571, acc 0.64
2017-12-15T20:30:28.516296: step 600, softmax_loss 62.8308, acc 0.64
2017-12-15T20:31:04.541570: step 800, softmax_loss 45.9002, acc 0.66
2017-12-15T20:31:40.577768: step 1000, softmax_loss 32.5254, acc 0.76
2017-12-15T20:32:16.438919: step 1200, softmax_loss 45.6278, acc 0.72
2017-12-15T20:32:52.357126: step 1400, softmax_loss 48.9877, acc 0.54
2017-12-15T20:33:28.200362: step 1600, softmax_loss 41.1167, acc 0.7
2017-12-15T20:34:04.100462: step 1800, softmax_loss 36.2977, acc 0.7
2017-12-15T20:34:40.000629: step 2000, softmax_loss 35.8009, acc 0.76
2017-12-15T20:35:15.884467: step 2200, softmax_loss 40.6179, acc 0.68
2017-12-15T20:35:51.742711: step 2400, softmax_loss 41.8828, acc 0.64
2017-12-15T20:36:27.637677: step 2600, softmax_loss 35.3594, acc 0.76
2017-12-15T20:37:03.526682: step 2800, softmax_loss 39.571, acc 0.68
2017-12-15T20:37:39.423808: step 3000, softmax_loss 48.9924, acc 0.6
2017-12-15T20:38:15.343263: step 3200, softmax_loss 37.4138, acc 0.68
2017-12-15T20:38:51.240242: step 3400, softmax_loss 33.055, acc 0.76
2017-12-15T20:39:27.198786: step 3600, softmax_loss 49.6605, acc 0.66
2017-12-15T20:40:03.564983: step 3800, softmax_loss 45.1851, acc 0.7
2017-12-15T20:40:39.552482: step 4000, softmax_loss 41.1454, acc 0.68
2017-12-15T20:41:15.426497: step 4200, softmax_loss 24.9318, acc 0.84
2017-12-15T20:41:51.503516: step 4400, softmax_loss 36.6871, acc 0.76
2017-12-15T20:42:27.424111: step 4600, softmax_loss 46.7437, acc 0.7
2017-12-15T20:43:03.449876: step 4800, softmax_loss 37.1176, acc 0.76
2017-12-15T20:43:39.335517: step 5000, softmax_loss 22.6507, acc 0.88
2017-12-15T20:44:15.294079: step 5200, softmax_loss 48.0955, acc 0.62
2017-12-15T20:44:51.172078: step 5400, softmax_loss 26.0948, acc 0.82
2017-12-15T20:45:27.048397: step 5600, softmax_loss 39.6772, acc 0.74
2017-12-15T20:46:02.898234: step 5800, softmax_loss 41.0019, acc 0.68
2017-12-15T20:46:38.784457: step 6000, softmax_loss 31.0464, acc 0.76
2017-12-15T20:47:14.850820: step 6200, softmax_loss 37.4369, acc 0.68
2017-12-15T20:47:50.681279: step 6400, softmax_loss 33.6695, acc 0.7
2017-12-15T20:48:26.590454: step 6600, softmax_loss 40.931, acc 0.7
2017-12-15T20:49:02.524724: step 6800, softmax_loss 37.2939, acc 0.72
2017-12-15T20:49:38.503162: step 7000, softmax_loss 20.643, acc 0.86
2017-12-15T20:50:14.347699: step 7200, softmax_loss 31.5041, acc 0.7
2017-12-15T20:50:50.303930: step 7400, softmax_loss 31.1642, acc 0.74
2017-12-15T20:51:26.251049: step 7600, softmax_loss 40.1898, acc 0.68
2017-12-15T20:52:02.156990: step 7800, softmax_loss 45.8613, acc 0.58
2017-12-15T20:52:38.122197: step 8000, softmax_loss 24.568, acc 0.78
2017-12-15T20:53:14.000471: step 8200, softmax_loss 29.0005, acc 0.76
2017-12-15T20:53:49.944473: step 8400, softmax_loss 36.4223, acc 0.7
2017-12-15T20:54:25.873159: step 8600, softmax_loss 25.9577, acc 0.76
2017-12-15T20:55:01.865183: step 8800, softmax_loss 43.4655, acc 0.66
2017-12-15T20:55:37.788813: step 9000, softmax_loss 29.6343, acc 0.74
2017-12-15T20:56:13.674036: step 9200, softmax_loss 50.6362, acc 0.66
2017-12-15T20:56:49.553012: step 9400, softmax_loss 28.0962, acc 0.8
2017-12-15T20:57:25.764092: step 9600, softmax_loss 42.476, acc 0.68
2017-12-15T20:58:01.897049: step 9800, softmax_loss 25.728, acc 0.82
2017-12-15T20:58:37.800377: step 10000, softmax_loss 26.265, acc 0.7
2017-12-15T20:59:13.766028: step 10200, softmax_loss 21.0585, acc 0.88
2017-12-15T20:59:49.690702: step 10400, softmax_loss 31.5119, acc 0.66
2017-12-15T21:00:25.654879: step 10600, softmax_loss 23.3067, acc 0.86
2017-12-15T21:01:01.639168: step 10800, softmax_loss 29.0149, acc 0.72
2017-12-15T21:01:37.556562: step 11000, softmax_loss 19.3262, acc 0.82
2017-12-15T21:02:13.448666: step 11200, softmax_loss 27.152, acc 0.8
2017-12-15T21:02:49.388452: step 11400, softmax_loss 24.2803, acc 0.78
2017-12-15T21:03:25.317855: step 11600, softmax_loss 41.0929, acc 0.58
2017-12-15T21:04:01.175831: step 11800, softmax_loss 33.6761, acc 0.72
2017-12-15T21:04:37.155950: step 12000, softmax_loss 32.0837, acc 0.76
2017-12-15T21:05:13.087723: step 12200, softmax_loss 21.9759, acc 0.82
2017-12-15T21:05:49.182218: step 12400, softmax_loss 39.0331, acc 0.64
2017-12-15T21:06:25.150551: step 12600, softmax_loss 33.8373, acc 0.66
2017-12-15T21:07:01.123220: step 12800, softmax_loss 22.6204, acc 0.84
2017-12-15T21:07:36.988236: step 13000, softmax_loss 31.0878, acc 0.64
2017-12-15T21:08:12.946378: step 13200, softmax_loss 22.1681, acc 0.78
2017-12-15T21:08:48.949844: step 13400, softmax_loss 30.0532, acc 0.72
2017-12-15T21:09:24.902102: step 13600, softmax_loss 22.615, acc 0.82
2017-12-15T21:10:00.843985: step 13800, softmax_loss 35.6667, acc 0.72
2017-12-15T21:10:36.800494: step 14000, softmax_loss 23.9789, acc 0.82
2017-12-15T21:11:12.759695: step 14200, softmax_loss 33.4807, acc 0.68
2017-12-15T21:11:48.755524: step 14400, softmax_loss 24.7101, acc 0.76
2017-12-15T21:12:24.664608: step 14600, softmax_loss 20.7017, acc 0.8
2017-12-15T21:13:00.642683: step 14800, softmax_loss 26.0683, acc 0.76
2017-12-15T21:13:36.709266: step 15000, softmax_loss 18.132, acc 0.78
2017-12-15T21:14:12.686025: step 15200, softmax_loss 26.5192, acc 0.8
2017-12-15T21:14:48.808064: step 15400, softmax_loss 30.1272, acc 0.74
2017-12-15T21:15:24.933379: step 15600, softmax_loss 29.113, acc 0.74
2017-12-15T21:16:01.220721: step 15800, softmax_loss 24.3684, acc 0.84
2017-12-15T21:16:37.169595: step 16000, softmax_loss 23.0406, acc 0.76
saving model
have saved model to ./model/ATT_GRU_model-16000
2017-12-15T21:17:16.572325: step 16200, softmax_loss 31.933, acc 0.72
2017-12-15T21:17:52.551540: step 16400, softmax_loss 19.8004, acc 0.78
2017-12-15T21:18:28.433685: step 16600, softmax_loss 12.422, acc 0.92
2017-12-15T21:19:04.423150: step 16800, softmax_loss 22.5175, acc 0.74
2017-12-15T21:19:40.392328: step 17000, softmax_loss 20.1021, acc 0.82
2017-12-15T21:20:16.327445: step 17200, softmax_loss 18.0383, acc 0.86
2017-12-15T21:20:52.301821: step 17400, softmax_loss 26.8915, acc 0.76
2017-12-15T21:21:28.249158: step 17600, softmax_loss 17.9814, acc 0.86
2017-12-15T21:22:04.135880: step 17800, softmax_loss 24.5634, acc 0.78
2017-12-15T21:22:40.106389: step 18000, softmax_loss 13.6954, acc 0.88
saving model
have saved model to ./model/ATT_GRU_model-18000
2017-12-15T21:23:18.808572: step 18200, softmax_loss 19.2404, acc 0.78
2017-12-15T21:23:54.862758: step 18400, softmax_loss 22.4073, acc 0.82
2017-12-15T21:24:30.882892: step 18600, softmax_loss 14.634, acc 0.9
2017-12-15T21:25:06.828586: step 18800, softmax_loss 19.2366, acc 0.86
2017-12-15T21:25:42.746789: step 19000, softmax_loss 21.214, acc 0.82
2017-12-15T21:26:18.625664: step 19200, softmax_loss 17.7273, acc 0.88
2017-12-15T21:26:54.573746: step 19400, softmax_loss 12.9359, acc 0.88
2017-12-15T21:27:30.539381: step 19600, softmax_loss 20.0613, acc 0.84
2017-12-15T21:28:06.476226: step 19800, softmax_loss 29.5278, acc 0.74
2017-12-15T21:28:42.498562: step 20000, softmax_loss 23.5175, acc 0.82
saving model
have saved model to ./model/ATT_GRU_model-20000
2017-12-15T21:29:21.170754: step 20200, softmax_loss 27.5576, acc 0.78
2017-12-15T21:29:57.097439: step 20400, softmax_loss 21.4152, acc 0.8
2017-12-15T21:30:33.064940: step 20600, softmax_loss 9.89864, acc 0.94
2017-12-15T21:31:09.024196: step 20800, softmax_loss 26.4107, acc 0.8
2017-12-15T21:31:45.029936: step 21000, softmax_loss 16.2812, acc 0.9
2017-12-15T21:32:21.021503: step 21200, softmax_loss 16.7031, acc 0.88
2017-12-15T21:32:56.959260: step 21400, softmax_loss 14.2739, acc 0.9
