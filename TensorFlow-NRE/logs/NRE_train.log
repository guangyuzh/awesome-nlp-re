reading wordembedding
reading training data
2017-12-09T21:28:47.248175: step 200, softmax_loss 66.0244, acc 0.68
2017-12-09T21:29:31.353472: step 400, softmax_loss 49.2162, acc 0.68
2017-12-09T21:30:15.443016: step 600, softmax_loss 61.7764, acc 0.64
2017-12-09T21:30:59.620857: step 800, softmax_loss 48.4562, acc 0.68
2017-12-09T21:31:43.836282: step 1000, softmax_loss 33.4091, acc 0.72
2017-12-09T21:32:28.047646: step 1200, softmax_loss 49.6716, acc 0.74
2017-12-09T21:33:12.642618: step 1400, softmax_loss 48.9165, acc 0.58
2017-12-09T21:33:57.687832: step 1600, softmax_loss 36.6651, acc 0.72
2017-12-09T21:34:42.972065: step 1800, softmax_loss 39.5144, acc 0.68
2017-12-09T21:35:28.166286: step 2000, softmax_loss 39.3228, acc 0.68
2017-12-09T21:36:13.374207: step 2200, softmax_loss 42.1895, acc 0.66
2017-12-09T21:36:58.630737: step 2400, softmax_loss 41.4207, acc 0.66
2017-12-09T21:37:43.819604: step 2600, softmax_loss 35.6387, acc 0.7
2017-12-09T21:38:28.871375: step 2800, softmax_loss 40.9902, acc 0.68
2017-12-09T21:39:13.849726: step 3000, softmax_loss 48.5058, acc 0.62
2017-12-09T21:39:58.953007: step 3200, softmax_loss 36.8029, acc 0.7
2017-12-09T21:40:44.033252: step 3400, softmax_loss 33.0509, acc 0.76
2017-12-09T21:41:29.199460: step 3600, softmax_loss 47.7627, acc 0.74
2017-12-09T21:42:14.387943: step 3800, softmax_loss 47.9325, acc 0.66
2017-12-09T21:42:59.456513: step 4000, softmax_loss 38.2218, acc 0.68
2017-12-09T21:43:44.606283: step 4200, softmax_loss 24.5159, acc 0.82
2017-12-09T21:44:29.769965: step 4400, softmax_loss 36.8471, acc 0.74
2017-12-09T21:45:14.954516: step 4600, softmax_loss 46.071, acc 0.64
2017-12-09T21:46:00.079346: step 4800, softmax_loss 35.1267, acc 0.78
2017-12-09T21:46:45.276316: step 5000, softmax_loss 22.641, acc 0.86
2017-12-09T21:47:30.493007: step 5200, softmax_loss 46.5506, acc 0.64
2017-12-09T21:48:15.668986: step 5400, softmax_loss 25.811, acc 0.82
2017-12-09T21:49:00.779215: step 5600, softmax_loss 38.3061, acc 0.72
2017-12-09T21:49:45.836611: step 5800, softmax_loss 39.5219, acc 0.68
2017-12-09T21:50:31.075723: step 6000, softmax_loss 31.4813, acc 0.76
2017-12-09T21:51:16.115338: step 6200, softmax_loss 36.02, acc 0.66
2017-12-09T21:52:01.258671: step 6400, softmax_loss 32.5182, acc 0.66
2017-12-09T21:52:46.365470: step 6600, softmax_loss 37.7875, acc 0.66
2017-12-09T21:53:31.435336: step 6800, softmax_loss 36.7066, acc 0.74
2017-12-09T21:54:16.447837: step 7000, softmax_loss 21.9176, acc 0.82
2017-12-09T21:55:01.545319: step 7200, softmax_loss 34.4047, acc 0.7
2017-12-09T21:55:46.679791: step 7400, softmax_loss 28.8912, acc 0.72
2017-12-09T21:56:31.754506: step 7600, softmax_loss 42.2298, acc 0.66
2017-12-09T21:57:16.877260: step 7800, softmax_loss 47.3096, acc 0.56
2017-12-09T21:58:02.129401: step 8000, softmax_loss 25.3115, acc 0.86
2017-12-09T21:58:47.217102: step 8200, softmax_loss 29.7631, acc 0.76
2017-12-09T21:59:32.353993: step 8400, softmax_loss 40.6783, acc 0.64
2017-12-09T22:00:17.544569: step 8600, softmax_loss 23.4374, acc 0.78
2017-12-09T22:01:02.612668: step 8800, softmax_loss 37.2317, acc 0.72
2017-12-09T22:01:47.903753: step 9000, softmax_loss 28.083, acc 0.76
2017-12-09T22:02:33.044962: step 9200, softmax_loss 41.2032, acc 0.72
2017-12-09T22:03:18.315804: step 9400, softmax_loss 24.8116, acc 0.78
2017-12-09T22:04:03.818376: step 9600, softmax_loss 38.5076, acc 0.7
2017-12-09T22:04:49.133469: step 9800, softmax_loss 24.9419, acc 0.78
2017-12-09T22:05:34.697475: step 10000, softmax_loss 26.1349, acc 0.76
2017-12-09T22:06:20.282684: step 10200, softmax_loss 22.2137, acc 0.88
2017-12-09T22:07:05.635023: step 10400, softmax_loss 32.3092, acc 0.64
2017-12-09T22:07:51.241612: step 10600, softmax_loss 24.5662, acc 0.8
2017-12-09T22:08:36.549372: step 10800, softmax_loss 30.2406, acc 0.74
2017-12-09T22:09:22.196566: step 11000, softmax_loss 23.1261, acc 0.82
2017-12-09T22:10:07.667037: step 11200, softmax_loss 25.0525, acc 0.8
2017-12-09T22:10:53.040376: step 11400, softmax_loss 28.0181, acc 0.78
2017-12-09T22:11:38.697326: step 11600, softmax_loss 40.2373, acc 0.66
2017-12-09T22:12:24.054963: step 11800, softmax_loss 32.4486, acc 0.72
2017-12-09T22:13:09.615466: step 12000, softmax_loss 31.7152, acc 0.74
2017-12-09T22:13:54.926993: step 12200, softmax_loss 23.2316, acc 0.8
2017-12-09T22:14:40.442760: step 12400, softmax_loss 32.29, acc 0.72
2017-12-09T22:15:25.983355: step 12600, softmax_loss 33.5252, acc 0.68
2017-12-09T22:16:11.249265: step 12800, softmax_loss 21.0959, acc 0.86
2017-12-09T22:16:56.749055: step 13000, softmax_loss 26.9508, acc 0.74
2017-12-09T22:17:42.057262: step 13200, softmax_loss 20.7691, acc 0.84
2017-12-09T22:18:27.561734: step 13400, softmax_loss 23.6238, acc 0.76
2017-12-09T22:19:13.080963: step 13600, softmax_loss 21.9073, acc 0.84
2017-12-09T22:19:58.466101: step 13800, softmax_loss 26.1687, acc 0.8
2017-12-09T22:20:44.016291: step 14000, softmax_loss 20.5322, acc 0.8
2017-12-09T22:21:29.284808: step 14200, softmax_loss 29.6822, acc 0.68
2017-12-09T22:22:14.897156: step 14400, softmax_loss 24.1698, acc 0.8
2017-12-09T22:23:00.448304: step 14600, softmax_loss 18.1192, acc 0.84
2017-12-09T22:23:45.656937: step 14800, softmax_loss 20.6516, acc 0.84
2017-12-09T22:24:31.137749: step 15000, softmax_loss 17.8509, acc 0.76
2017-12-09T22:25:16.475973: step 15200, softmax_loss 23.0269, acc 0.78
2017-12-09T22:26:02.045481: step 15400, softmax_loss 23.2113, acc 0.76
2017-12-09T22:26:47.180374: step 15600, softmax_loss 28.5104, acc 0.72
2017-12-09T22:27:32.605276: step 15800, softmax_loss 15.162, acc 0.92
2017-12-09T22:28:17.986360: step 16000, softmax_loss 34.0411, acc 0.68
saving model
have saved model to ./model/ATT_GRU_model-16000
2017-12-09T22:29:06.861587: step 16200, softmax_loss 32.4506, acc 0.74
2017-12-09T22:29:51.285169: step 16400, softmax_loss 20.981, acc 0.82
2017-12-09T22:30:35.772197: step 16600, softmax_loss 22.7881, acc 0.84
2017-12-09T22:31:20.086937: step 16800, softmax_loss 20.7726, acc 0.8
2017-12-09T22:32:04.275536: step 17000, softmax_loss 18.7378, acc 0.88
2017-12-09T22:32:48.810127: step 17200, softmax_loss 19.9104, acc 0.84
2017-12-09T22:33:33.260558: step 17400, softmax_loss 29.7246, acc 0.78
2017-12-09T22:34:17.857321: step 17600, softmax_loss 17.8148, acc 0.82
2017-12-09T22:35:02.344440: step 17800, softmax_loss 24.5872, acc 0.78
2017-12-09T22:35:46.827361: step 18000, softmax_loss 15.238, acc 0.88
saving model
have saved model to ./model/ATT_GRU_model-18000
2017-12-09T22:36:35.095534: step 18200, softmax_loss 15.9059, acc 0.86
2017-12-09T22:37:19.468708: step 18400, softmax_loss 16.331, acc 0.86
2017-12-09T22:38:03.927647: step 18600, softmax_loss 14.3995, acc 0.88
2017-12-09T22:38:48.309400: step 18800, softmax_loss 21.2541, acc 0.82
2017-12-09T22:39:32.722526: step 19000, softmax_loss 22.0267, acc 0.82
2017-12-09T22:40:17.119464: step 19200, softmax_loss 19.5371, acc 0.84
2017-12-09T22:41:01.546702: step 19400, softmax_loss 13.3558, acc 0.86
2017-12-09T22:41:45.926500: step 19600, softmax_loss 19.7639, acc 0.84
2017-12-09T22:42:30.326655: step 19800, softmax_loss 25.3375, acc 0.78
2017-12-09T22:43:14.723328: step 20000, softmax_loss 21.4487, acc 0.82
saving model
have saved model to ./model/ATT_GRU_model-20000
2017-12-09T22:44:02.609903: step 20200, softmax_loss 19.739, acc 0.76
2017-12-09T22:44:46.993585: step 20400, softmax_loss 21.6483, acc 0.78
2017-12-09T22:45:31.381238: step 20600, softmax_loss 15.5134, acc 0.86
2017-12-09T22:46:15.734045: step 20800, softmax_loss 21.4731, acc 0.86
2017-12-09T22:47:00.134091: step 21000, softmax_loss 7.49148, acc 0.96
2017-12-09T22:47:44.437577: step 21200, softmax_loss 14.6838, acc 0.86
2017-12-09T22:48:28.808141: step 21400, softmax_loss 16.2103, acc 0.88
